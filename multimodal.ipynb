{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717dfe5c-7327-4ec1-96a3-7d6737ded619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d334ba5e-06ee-4d1a-81c2-339695d567f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14370416-2f05-4850-adde-bdb92a02a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a estratégia é:\n",
    "#1)experimentar DenseNet121-res224-all transfer learning (img) + BlueBERT feature extraction (text) -> denselayer\n",
    "#se a anterior não funcionar (não foi preciso!!!)\n",
    "#2)experimentar DenseNet121-res224-chex ou mimic_ch transfer learning (img) + BlueBERT feature extraction (text) -> denselayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69ee8c0e-1b36-45d2-8738-32fe4cf2583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms as transforms\n",
    "import torchxrayvision as xrv\n",
    "import time\n",
    "from collections import Counter\n",
    "import os\n",
    "import skimage\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1ed872f-a384-42cd-b0e1-ded570387680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/ieee8023/covid-chestxray-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af04fc5c-dddf-41b8-bf03-7877c4bd86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xrv.datasets.COVID19_Dataset(imgpath=\"covid-chestxray-dataset/images/\",csvpath=\"covid-chestxray-dataset/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a1797b8-7016-4fce-893b-755187bca2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aspergillosis': {np.float32(0.0): 534, np.float32(1.0): 1},\n",
      " 'Aspiration': {np.float32(0.0): 534, np.float32(1.0): 1},\n",
      " 'Bacterial': {np.float32(0.0): 487, np.float32(1.0): 48},\n",
      " 'COVID-19': {np.float32(0.0): 193, np.float32(1.0): 342},\n",
      " 'Chlamydophila': {np.float32(0.0): 534, np.float32(1.0): 1},\n",
      " 'Fungal': {np.float32(0.0): 512, np.float32(1.0): 23},\n",
      " 'H1N1': {np.float32(0.0): 534, np.float32(1.0): 1},\n",
      " 'Herpes ': {np.float32(0.0): 532, np.float32(1.0): 3},\n",
      " 'Influenza': {np.float32(0.0): 531, np.float32(1.0): 4},\n",
      " 'Klebsiella': {np.float32(0.0): 526, np.float32(1.0): 9},\n",
      " 'Legionella': {np.float32(0.0): 526, np.float32(1.0): 9},\n",
      " 'Lipoid': {np.float32(0.0): 527, np.float32(1.0): 8},\n",
      " 'MERS-CoV': {np.float32(0.0): 527, np.float32(1.0): 8},\n",
      " 'MRSA': {np.float32(0.0): 534, np.float32(1.0): 1},\n",
      " 'Mycoplasma': {np.float32(0.0): 530, np.float32(1.0): 5},\n",
      " 'No Finding': {np.float32(0.0): 520, np.float32(1.0): 15},\n",
      " 'Nocardia': {np.float32(0.0): 531, np.float32(1.0): 4},\n",
      " 'Pneumocystis': {np.float32(0.0): 513, np.float32(1.0): 22},\n",
      " 'Pneumonia': {np.float32(0.0): 26, np.float32(1.0): 509},\n",
      " 'SARS': {np.float32(0.0): 519, np.float32(1.0): 16},\n",
      " 'Staphylococcus': {np.float32(0.0): 534, np.float32(1.0): 1},\n",
      " 'Streptococcus': {np.float32(0.0): 518, np.float32(1.0): 17},\n",
      " 'Tuberculosis': {np.float32(0.0): 524, np.float32(1.0): 11},\n",
      " 'Varicella': {np.float32(0.0): 530, np.float32(1.0): 5},\n",
      " 'Viral': {np.float32(0.0): 157, np.float32(1.0): 378}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "COVID19_Dataset num_samples=535 views=['PA', 'AP'] data_aug=None"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59441a60-b50c-4b9e-8745-3730864cb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36667808-920d-4fbf-9a41-edb9d3de8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Shape da imagem no índice {i}: {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d029e81c-e9e7-4118-acb2-be0c802964cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian(img):\n",
    "  return cv2.Laplacian(img, cv2.CV_64F).var() #aplica filtro laplaciano para detectar bordas das imagens\n",
    "    #faz isso através da variância: regiões mais nítidas (i.e. bordas) têm maior variância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79764903-a650-4eba-9170-867fc5d8edd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadtreeNode:\n",
    "    #Nó de uma Quadtree para fusão de imagens\n",
    "    def __init__(self, x, y, width, height, images): #canto sup esquerdo\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.images = images  #lista de imagens de entrada\n",
    "        self.children = []\n",
    "        self.best_image_index = -1 #índice da imagem com o melhor foco para este nó\n",
    "\n",
    "    def split(self, threshold_size):\n",
    "        #Divide o nó em quatro filhos (/2 cada dimensão) se o tamanho for maior que o limite\n",
    "        if self.width <= threshold_size or self.height <= threshold_size:\n",
    "            # Se o nó é pequeno o suficiente, encontre a melhor imagem para ele\n",
    "            self.find_best_focus()\n",
    "            return\n",
    "\n",
    "        half_width = self.width // 2\n",
    "        half_height = self.height // 2\n",
    "\n",
    "        #cri os quatro filhos\n",
    "        self.children.append(QuadtreeNode(self.x, self.y, half_width, half_height, self.images))\n",
    "        self.children.append(QuadtreeNode(self.x + half_width, self.y, self.width - half_width, half_height, self.images))\n",
    "        self.children.append(QuadtreeNode(self.x, self.y + half_height, half_width, self.height - half_height, self.images))\n",
    "        self.children.append(QuadtreeNode(self.x + half_width, self.y + half_height, self.width - half_width, self.height - half_height, self.images))\n",
    "\n",
    "        #recursivamente divida os filhos\n",
    "        for child in self.children:\n",
    "            child.split(threshold_size)\n",
    "\n",
    "    def find_best_focus(self):\n",
    "        #encontra a imagem com a maior medida de foco (maior laplacian) para a região do nó\n",
    "        max_focus = -1\n",
    "        best_index = -1\n",
    "\n",
    "        for i, img in enumerate(self.images):\n",
    "            #extrai a região de interesse (ROI) da imagem\n",
    "            roi = img[self.y : self.y + self.height, self.x : self.x + self.width]\n",
    "\n",
    "            #garante que o ROI não esteja vazio antes de calcular a medida de foco\n",
    "            if roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            focus_measure = laplacian(roi)\n",
    "            if focus_measure > max_focus:\n",
    "                max_focus = focus_measure\n",
    "                best_index = i\n",
    "        self.best_image_index = best_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e5df017-042a-4140-aa35-f2589f556eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadtree_focus_fusion(images, threshold_size=32):\n",
    "    #realiza a fusão de imagens multifocais usando uma Quadtree\n",
    "    if not images:\n",
    "        return None\n",
    "\n",
    "    #as imagens devem ter o mesmo tamanho\n",
    "    height, width = images[0].shape\n",
    "\n",
    "    #cria uma imagem de saída base\n",
    "    fused_image = np.zeros_like(images[0])\n",
    "\n",
    "    #adiciona o nó raiz da Quadtree\n",
    "    root = QuadtreeNode(0, 0, width, height, images)\n",
    "    root.split(threshold_size)\n",
    "\n",
    "    #auxiliar para reconstruir a imagem fusionada\n",
    "    def reconstruct_image(node, output_image):\n",
    "        #se o nó tem uma melhor imagem definida, copia para output_image\n",
    "        #se não, visita os filhos\n",
    "        if node.best_image_index != -1:\n",
    "            #se é um nó folha (ou um nó onde a melhor imagem foi encontrada)\n",
    "            best_img_roi = node.images[node.best_image_index][node.y : node.y + node.height, node.x : node.x + node.width]\n",
    "            output_image[node.y : node.y + node.height, node.x : node.x + node.width] = best_img_roi\n",
    "        else:\n",
    "            #se não é um nó folha, aprofunda na árvore \n",
    "            for child in node.children:\n",
    "                reconstruct_image(child, output_image)\n",
    "\n",
    "    reconstruct_image(root, fused_image)\n",
    "    return fused_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf7d315-4726-4c0c-ba16-deb67a743404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tudo isso é feito para aumentar sintéticamente o tamanho do dataset. Ele é dobrado de tamanho,\n",
    "#pois preservarmos a imagem e sua versão fusionada com a blurred\n",
    "#encontramos usos dessa técnica na literatura, em contextos semelhantes ao nosso.\n",
    "#referencia: https://www.sciencedirect.com/science/article/abs/pii/S0010482524015920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfd3f3b8-8229-44ac-96f0-a5b40d6b4a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images_and_prepare_data(data, save_dir=\"covid_images_organized\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    image_paths = []\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        #obtem imagem (normalizada entre 0-1)\n",
    "        img = data[i][\"img\"]\n",
    "        img = np.squeeze(img)\n",
    "\n",
    "        #converte para uint8 e PIL\n",
    "        img_uint8 = (img * 255).astype(np.uint8)\n",
    "        pil_img = Image.fromarray(img_uint8)\n",
    "\n",
    "        #obtem filename do DataFrame\n",
    "        filename = data.csv.iloc[i]['filename']\n",
    "        if not isinstance(filename, str):  #pula se for NaN ou inválido\n",
    "            continue\n",
    "\n",
    "        #caminho completo para salvar\n",
    "        filepath = os.path.join(save_dir, filename)\n",
    "        pil_img.save(filepath)\n",
    "        image_paths.append(filepath)\n",
    "\n",
    "        # Texto clínico associado, string ou vazio\n",
    "        clinical_note = data.csv.iloc[i]['clinical_notes']\n",
    "        texts.append(clinical_note if isinstance(clinical_note, str) else \"\")\n",
    "\n",
    "        # Label binária para COVID-19\n",
    "        lab_series = pd.Series(dict(zip(data.pathologies, data[i][\"lab\"])))\n",
    "        labels.append(lab_series['COVID-19'])\n",
    "\n",
    "        #blur\n",
    "        mean_filter = np.ones((3, 3), dtype=np.float32) / 9.0\n",
    "        img_blurred = cv2.filter2D(img_uint8, -1, mean_filter)\n",
    "\n",
    "        #aplica quadtree focus fusion\n",
    "        fused_img = quadtree_focus_fusion([img_uint8, img_blurred], threshold_size=32)\n",
    "\n",
    "        #salvar imagem fusionada\n",
    "        filepath_fused = os.path.join(save_dir, f\"fused_{filename}\")\n",
    "        fused_pil = Image.fromarray(fused_img)\n",
    "        fused_pil.save(filepath_fused)\n",
    "\n",
    "        image_paths.append(filepath_fused)\n",
    "        texts.append(clinical_note if isinstance(clinical_note, str) else \"\")\n",
    "        labels.append(lab_series['COVID-19'])\n",
    "\n",
    "    return image_paths, texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ff37382-33e8-4d59-9a0b-3bc65209786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_split(image_paths, texts, labels, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    trainval_img, test_img, trainval_txt, test_txt, trainval_lab, test_lab = train_test_split(\n",
    "        image_paths, texts, labels, test_size=test_size, random_state=random_state, stratify=labels\n",
    "    )\n",
    "\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    train_img, val_img, train_txt, val_txt, train_lab, val_lab = train_test_split(\n",
    "        trainval_img, trainval_txt, trainval_lab, test_size=val_ratio, random_state=random_state, stratify=trainval_lab\n",
    "    )\n",
    "\n",
    "    return (train_img, train_txt, train_lab), (val_img, val_txt, val_lab), (test_img, test_txt, test_lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "864131db-93fb-40ea-9793-0683f75b366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1), #xrays are b&w but densenet expects rgb\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(degrees=9),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(), #image (format PIL) -> tensor (format pytorch)\n",
    "        #normalizar por 255 - ToTensor já faz isso automatico\n",
    "        transforms.Normalize([0.5], [0.5])  \n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])  \n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize([0.5], [0.5])  \n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "049e61f7-35e7-4c28-abda-50230e064967",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths, texts, labels = save_images_and_prepare_data(data, save_dir=\"covid_images_organized\")\n",
    "\n",
    "(train_img, train_txt, train_lab), (val_img, val_txt, val_lab), (test_img, test_txt, test_lab) = create_train_val_test_split(\n",
    "    image_paths, texts, labels, test_size=0.2, val_size=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0c998c5-8538-469b-aadc-826522be78f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1070\n"
     ]
    }
   ],
   "source": [
    "print(len(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7221792f-6773-4e65-8d08-4c5c76cab7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0828942-9b77-4316-ae02-8ad0249748db",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60fc744c-0dea-4d2c-a130-90adc50366fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criando dataset multimodal, ou seja, que comparta as imagens e os textos\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, image_paths, texts, labels, tokenizer, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #imagem\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('L')  #abre imagem em greyscale\n",
    "    \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Texto\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label = torch.tensor(self.labels[idx]).long()\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'image': image,\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0139db1-1508-4afc-97af-3d7ed5df31ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cria os datasets e dataloaders\n",
    "train_dataset = MultimodalDataset(train_img, train_txt, train_lab, tokenizer, transform=image_transforms['train'])\n",
    "val_dataset = MultimodalDataset(val_img, val_txt, val_lab, tokenizer, transform=image_transforms['val'])\n",
    "test_dataset = MultimodalDataset(test_img, test_txt, test_lab, tokenizer, transform=image_transforms['test'])\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=24, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "164b02cc-f9c6-486e-ae90-1570c62a5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduzimos um pouco o threshold de decisão, para favorecer a classificação como covid em casos de dúvida\n",
    "#evitando falsos negativos e maximizando o recall, como queríamos\n",
    "threshold = 0.475"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a027dcb-e1a6-4df6-9c9d-5d75ad2cbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mesma função que usamos no trabalho todo, mas agora adaptada à multimodalidade\n",
    "def train_and_validate_multimodal(model, loss_criterion, optimizer, epochs, dataset=\"multimodal\",\n",
    "                                  train_data_loader=train_data_loader, val_data_loader=val_data_loader,\n",
    "                                  train_data_size=len(train_dataset), val_data_size=len(val_dataset), device=device):\n",
    "\n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_loss = 100000.0\n",
    "    best_epoch = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_preds = []\n",
    "        train_labels_list = []\n",
    "\n",
    "        valid_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels_list = []\n",
    "\n",
    "        #train\n",
    "        for i, batch in enumerate(train_data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, images=images).squeeze()\n",
    "\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            #\"versão\" comum do código, usando threshold de 0.5\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            preds = preds.cpu().numpy()\n",
    "            labels_np = labels.cpu().numpy()\n",
    "\n",
    "            train_preds.extend(preds)\n",
    "            train_labels_list.extend(labels_np)\n",
    "\n",
    "            \n",
    "            # \"versão\" do código com o threshold definido\n",
    "            # probs = torch.softmax(outputs, dim=1)[:, 1]  \n",
    "            # preds = (probs >= threshold).long().cpu().numpy()\n",
    "\n",
    "            # labels_np = labels.cpu().numpy()\n",
    "            # val_preds.extend(preds)\n",
    "            # val_labels_list.extend(labels_np)\n",
    "\n",
    "        avg_train_loss = train_loss / train_data_size\n",
    "        train_accuracy = accuracy_score(train_labels_list, train_preds)\n",
    "        train_recall = recall_score(train_labels_list, train_preds)\n",
    "        train_precision = precision_score(train_labels_list, train_preds)\n",
    "        train_f1 = f1_score(train_labels_list, train_preds)\n",
    "\n",
    "        #val\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for j, batch in enumerate(val_data_loader):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                images = batch['image'].to(device)\n",
    "                labels = batch['label'].to(device).long()\n",
    "\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, images=images).squeeze()\n",
    "\n",
    "                loss = loss_criterion(outputs, labels)\n",
    "\n",
    "                valid_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                preds = preds.cpu().numpy()\n",
    "                labels_np = labels.cpu().numpy()\n",
    "\n",
    "                val_preds.extend(preds)\n",
    "                val_labels_list.extend(labels_np)\n",
    "\n",
    "            avg_valid_loss = valid_loss / val_data_size\n",
    "            valid_accuracy = accuracy_score(val_labels_list, val_preds)\n",
    "            valid_recall = recall_score(val_labels_list, val_preds)\n",
    "            valid_precision = precision_score(val_labels_list, val_preds)\n",
    "            valid_f1 = f1_score(val_labels_list, val_preds)\n",
    "\n",
    "        if avg_valid_loss < best_loss:\n",
    "            best_loss = avg_valid_loss\n",
    "            best_epoch = epoch\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, train_accuracy, valid_accuracy, train_f1, valid_f1])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        print(f\"Epoch : {epoch:03d}, Training: Loss - {avg_train_loss:.4f}, Accuracy - {train_accuracy * 100:.2f}%, Precision - {train_precision:.4f}, Recall - {train_recall:.4f}, F1 - {train_f1:.4f} \\n\\t\\t\"\n",
    "              f\"Validation : Loss - {avg_valid_loss:.4f}, Accuracy - {valid_accuracy * 100:.2f}%, Precision - {valid_precision:.4f}, Recall - {valid_recall:.4f}, F1 - {valid_f1:.4f}, Time: {epoch_end - epoch_start:.4f}s\")\n",
    "        torch.save(model, dataset + '_model_' + str(epoch) + '.pt')\n",
    "\n",
    "    return model, history, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88b5f5e8-56fb-45f8-b199-f79a14643989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#computa a performance no dataset de teste\n",
    "def computeTestSetPerformance_multimodal(model, loss_criterion, test_data_loader=test_data_loader, \n",
    "                                         test_data_size=len(test_dataset), device=device):\n",
    "\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    test_true_positives = 0.0\n",
    "    test_false_negatives = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for j, batch in enumerate(test_data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, images=images)\n",
    "\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "            test_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "            #_, predictions = torch.max(outputs.data, 1) #de quando experimentamos o threshold de 0.5\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1] \n",
    "            predictions = (probs >= threshold).long()\n",
    "\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "            acc = correct_counts.sum().item() / input_ids.size(0)\n",
    "            test_acc += acc * input_ids.size(0)\n",
    "\n",
    "            test_true_positives += ((predictions == 1) & (labels == 1)).sum().item()\n",
    "            test_false_negatives += ((predictions == 0) & (labels == 1)).sum().item()\n",
    "\n",
    "           \n",
    "    avg_test_loss = test_loss / test_data_size\n",
    "    avg_test_acc = test_acc / test_data_size\n",
    "    test_recall = test_true_positives / (test_true_positives + test_false_negatives + 1e-7)\n",
    "\n",
    "    print(\"Test Loss: {:.4f}, Test Accuracy: {:.4f}%, Test Recall: {:.4f}\".format(avg_test_loss, avg_test_acc * 100, test_recall))\n",
    "\n",
    "    return avg_test_loss, avg_test_acc, test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de0eb838-d6fd-4743-bd71-5438420991dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faz a predição de uma instância em particular\n",
    "def predict_multimodal(model, test_image_name, clinical_note, tokenizer, device):\n",
    "\n",
    "    #pré-processamento da imagem de acordo com o treino\n",
    "    transform = image_transforms['test']\n",
    "    test_image = Image.open(test_image_name).convert('RGB')\n",
    "    plt.imshow(test_image)\n",
    "    test_image_tensor = transform(test_image).unsqueeze(0).to(device)\n",
    "\n",
    "    #pré-processamento do texto\n",
    "    encoded = tokenizer(clinical_note, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "    input_ids = encoded['input_ids'].to(device)\n",
    "    attention_mask = encoded['attention_mask'].to(device)\n",
    "\n",
    "    #passa pelas redes\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, images=test_image_tensor)\n",
    "\n",
    "        #não precisamos usar ela com threshold diferente do default, então não mudamos aqui\n",
    "        topk, topclass = out.topk(1, dim=1)\n",
    "        cls = idx_to_class[topclass.cpu().numpy()[0][0]]\n",
    "        score = topk.cpu().numpy()[0][0]\n",
    "\n",
    "        print(\"Prediction:\", cls, \", with probability:\", score)\n",
    "\n",
    "    return cls, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ef04ae2-40ec-43fe-b358-0a7b4e21f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#construindo classe do classificador \n",
    "class MultimodalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalNet, self).__init__()\n",
    "\n",
    "        #texto - BlueBERT\n",
    "        self.text_model = AutoModel.from_pretrained('bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12')\n",
    "        for param in self.text_model.parameters():\n",
    "            param.requires_grad = False  #feature extraction\n",
    "        self.text_fc = nn.Linear(768, 256)\n",
    "\n",
    "        #imagem - DenseNet121 pré treinada do torchxray vision\n",
    "        self.image_model = xrv.models.DenseNet(weights=\"densenet121-res224-all\")\n",
    "        for param in self.image_model.parameters():\n",
    "            param.requires_grad = False  #só transfer learning\n",
    "        \n",
    "        self.image_fc = nn.Linear(1024, 256)\n",
    "\n",
    "        #fully connected\n",
    "        self.fc_combined = nn.Sequential(\n",
    "            nn.Linear(512, 128), \n",
    "            nn.LayerNorm(128),  #porque usei layernorm e não batchnorm? explicação no final\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.55),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.60),\n",
    "\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embedding = text_outputs.last_hidden_state[:, 0, :]  #pega o output do embedding (vetor denso), antes de ele entrar nas camadas de classificação do BlueBERT\n",
    "        text_features = self.text_fc(text_embedding) #passa ele pra fully connected dessa rede\n",
    "\n",
    "        image_features = self.image_model.features(images) #pega o output das convoluções, ou seja, antes de entrar nas fc layers da densenet e fazer a classificação\n",
    "        image_features = image_features.view(image_features.size(0), -1)\n",
    "        image_features = self.image_fc(image_features) #manda esse output para a fully connected que construí ali em cima\n",
    "\n",
    "        #concatenação\n",
    "        combined = torch.cat((text_features, image_features), dim=1)\n",
    "        output = self.fc_combined(combined) \n",
    "\n",
    "        return output\n",
    "\n",
    "#explicação: sem usar nenhum tipo de normalização, a rede não aprendia muito: continuava naquilo de prever a classe majoritária sempre\n",
    "#como já tínhamos visto que a rede de texto funcionava bem, presumimos que as imagens estivessem \"confundindo a rede\" \n",
    "#- até por isso, tentamos o fine tuning da densenet\n",
    "#quando experimentávamos utilizar batch normalization, a rede overfitava muito fácil. \n",
    "#pelo que pesquisamos e buscamos entender, o que aconteceu foi: utilizamos batchs pequenos \n",
    "#(apesar de grandes pro nosso dataset, pois nossa quantidade de exemplos disponíveis era pequena)\n",
    "#isso atrapalhava a generalização, já que o batch norm depende muito do tamanho do batch\n",
    "#usar batchs maiores não era uma opção, porque cada batch já representava mais de 10% do dataset todo\n",
    "#então, procuramos por opções intermediárias, e descobrimos o layer normalization, que normaliza por instância ao longo das features\n",
    "#e, portanto, não dependia do batch. Experimentamos essa abordagem, e obtivemos resultados muito bons de cara. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2953a92c-9a77-4e27-94a8-681d9b09c5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalNet(\n",
       "  (text_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (text_fc): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (image_model): XRV-DenseNet121-densenet121-res224-all\n",
       "  (image_fc): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  (fc_combined): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.55, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.6, inplace=False)\n",
       "    (7): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultimodalNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ea06368-ff16-4af3-a0e9-aed572621593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tentamos um fine tuning da densenet aqui, por motivos já explicados\n",
    "#mas não deu certo, não melhorou a performance\n",
    "# for name, param in model.image_model.features.named_parameters():\n",
    "#     if \"denseblock4\" in name or \"norm5\" in name:\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa6647da-77f3-49da-bfb7-1438cc25fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = 2.4\n",
    "counter = Counter(train_lab)\n",
    "weight = (counter[0] / counter[1]) * boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd58d5b7-eea8-482e-9137-e75e5f52d09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3528183716075157\n"
     ]
    }
   ],
   "source": [
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c60951a-c4cf-411b-84a9-33429d4fecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#para os casos que tentamos fazer fine tuning da densenet, \n",
    "#usamos parametros distintos pras camadas dela a serem treinadas\n",
    "# image_params = []\n",
    "# other_params = []\n",
    "\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         if \"image_model.features.denseblock4\" in name or \"image_model.features.norm5\" in name:\n",
    "#             image_params.append(param)\n",
    "#         else:\n",
    "#             other_params.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "679f6f8b-1b7a-4aed-9204-0fe950d8ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([weight, 0.91]).to(device))\n",
    "# optimizer = torch.optim.AdamW([{'params': other_params, 'lr': 59e-7},{'params': image_params, 'lr': 51e-6}],\n",
    "#                               weight_decay=1e-4)\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=37e-6, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a47483b-43ac-4ca0-bee4-e07c2535ca52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 000, Training: Loss - 0.2534, Accuracy - 90.79%, Precision - 0.9399, Recall - 0.9144, F1 - 0.9270 \n",
      "\t\tValidation : Loss - 0.2447, Accuracy - 90.65%, Precision - 0.9531, Recall - 0.8971, F1 - 0.9242, Time: 205.9964s\n",
      "Epoch : 001, Training: Loss - 0.2619, Accuracy - 91.19%, Precision - 0.9172, Recall - 0.9478, F1 - 0.9322 \n",
      "\t\tValidation : Loss - 0.2618, Accuracy - 90.65%, Precision - 0.9677, Recall - 0.8824, F1 - 0.9231, Time: 203.5373s\n",
      "Epoch : 002, Training: Loss - 0.2333, Accuracy - 92.12%, Precision - 0.9526, Recall - 0.9228, F1 - 0.9374 \n",
      "\t\tValidation : Loss - 0.2304, Accuracy - 91.59%, Precision - 0.9683, Recall - 0.8971, F1 - 0.9313, Time: 201.5824s\n",
      "Epoch : 003, Training: Loss - 0.2276, Accuracy - 93.59%, Precision - 0.9499, Recall - 0.9499, F1 - 0.9499 \n",
      "\t\tValidation : Loss - 0.2654, Accuracy - 89.72%, Precision - 0.9672, Recall - 0.8676, F1 - 0.9147, Time: 203.0049s\n",
      "Epoch : 004, Training: Loss - 0.2370, Accuracy - 92.66%, Precision - 0.9380, Recall - 0.9478, F1 - 0.9429 \n",
      "\t\tValidation : Loss - 0.2407, Accuracy - 92.52%, Precision - 0.9688, Recall - 0.9118, F1 - 0.9394, Time: 202.1328s\n",
      "Epoch : 005, Training: Loss - 0.2206, Accuracy - 93.72%, Precision - 0.9557, Recall - 0.9457, F1 - 0.9507 \n",
      "\t\tValidation : Loss - 0.2355, Accuracy - 92.52%, Precision - 0.9688, Recall - 0.9118, F1 - 0.9394, Time: 201.9213s\n",
      "Epoch : 006, Training: Loss - 0.1992, Accuracy - 93.99%, Precision - 0.9578, Recall - 0.9478, F1 - 0.9528 \n",
      "\t\tValidation : Loss - 0.2481, Accuracy - 90.65%, Precision - 0.9677, Recall - 0.8824, F1 - 0.9231, Time: 209.4003s\n",
      "Epoch : 007, Training: Loss - 0.2327, Accuracy - 93.06%, Precision - 0.9495, Recall - 0.9415, F1 - 0.9455 \n",
      "\t\tValidation : Loss - 0.2277, Accuracy - 91.59%, Precision - 0.9538, Recall - 0.9118, F1 - 0.9323, Time: 204.5401s\n",
      "Epoch : 008, Training: Loss - 0.1995, Accuracy - 94.93%, Precision - 0.9623, Recall - 0.9582, F1 - 0.9603 \n",
      "\t\tValidation : Loss - 0.2566, Accuracy - 89.72%, Precision - 0.9831, Recall - 0.8529, F1 - 0.9134, Time: 205.5166s\n",
      "Epoch : 009, Training: Loss - 0.2094, Accuracy - 93.59%, Precision - 0.9675, Recall - 0.9311, F1 - 0.9489 \n",
      "\t\tValidation : Loss - 0.2110, Accuracy - 91.59%, Precision - 0.9538, Recall - 0.9118, F1 - 0.9323, Time: 204.1278s\n",
      "Epoch : 010, Training: Loss - 0.2173, Accuracy - 93.86%, Precision - 0.9482, Recall - 0.9562, F1 - 0.9522 \n",
      "\t\tValidation : Loss - 0.2270, Accuracy - 93.46%, Precision - 0.9841, Recall - 0.9118, F1 - 0.9466, Time: 210.7825s\n",
      "Epoch : 011, Training: Loss - 0.2279, Accuracy - 92.39%, Precision - 0.9528, Recall - 0.9269, F1 - 0.9397 \n",
      "\t\tValidation : Loss - 0.2291, Accuracy - 92.52%, Precision - 0.9688, Recall - 0.9118, F1 - 0.9394, Time: 202.2444s\n",
      "Epoch : 012, Training: Loss - 0.2252, Accuracy - 94.13%, Precision - 0.9448, Recall - 0.9645, F1 - 0.9545 \n",
      "\t\tValidation : Loss - 0.2422, Accuracy - 91.59%, Precision - 0.9836, Recall - 0.8824, F1 - 0.9302, Time: 201.9884s\n"
     ]
    }
   ],
   "source": [
    "trained_model, history, best_epoch = train_and_validate_multimodal(\n",
    "    model=model,\n",
    "    loss_criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    epochs=13\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "88859188-086e-4f65-930f-be93a986f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2063, Test Accuracy: 92.0561%, Test Recall: 0.8905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.20630931659279583, 0.9205607476635514, 0.8905109482551016)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeTestSetPerformance_multimodal(model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76b794ad-9052-4966-a7d2-3b0be696492a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(history)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history[:,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "history = np.array(history)\n",
    "plt.plot(history[:,0:2])\n",
    "plt.legend(['Train Loss', 'Validation Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0.3, 0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4f78df-42ad-4989-8d46-97638d4c4d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history[:,2:4])\n",
    "plt.legend(['Train Accuracy', 'Validation Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0138df21-c05d-4096-9d0b-a57ee1a0d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'multimodal_model_toupdate_sintectic_data_augmentation.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f305111c-79ca-43ff-a08e-ad3790f2fc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c59db4b-35a6-4caa-a0e1-4c683a99cad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41643b0d-e27e-4fe9-8775-2fd1cd58aeab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
